In the 1950s, our hardware was capable only of displaying grids of non-overlapping, fixed-sized small bitmaps.

In the 2020s, our hardware is capable of displaying overlapping, resizable rectangles.

In the 1950s, we invented "text" and "parsers for text".

In the 2020s, we continue to use "text" and "parsers for text", biased towards 1950s hardware.


In the 2020s, we have something called SVG, which is capable of expressing:
- rectangles
- lines
- groups
- ellipses
- text

In the 1950s, we invented something called "programming languages" that were based on the hardware-of-the-day.  "Programming languages" are a restricted form of prose.

In the 2020s, we *could* invent something called "technical diagrams" based on a restricted form of graphical art, consisting of, say, rectangles, ellipses, lines, text, groups, and, we *could* write parsers for such inventions.  "Technical diagrams" would be a restricted form of visual art.

Prose is based on clay-tablet technology.  Visual art is based on drawing.


In the 1950s, we only had ASCII.  In the 2020s we have Unicode for expressing small, grid-based bitmaps and we have SVG for expressing non-grid-based elements.

In the 1950s, we could only afford a single character to express the beginning and end of strings.  

In the 1950s we banished whitespace from identifiers to conserve ASCII and to appease nascent parsing technologies.

In the 1950s, we chose to use a notation based on clay-tablet technologies, called "text" and "characters", for programming.

In the 1950s, parsing was considered difficult. In the 2020s we have PEG and parser-generator DSLs like Ohm-JS.

Ohm-JS based parsers can parse general elements, but, we continue to use this technology to parse only characters.

In the 1950s, all parsing technologies pattern-matched sequences of characters. In the 2020s, we continue to use parsing technologies that pattern-match sequences of characters.

In the 1950s, backtracking was considered impolite.  In the 2020s, we have freely available backtracking notations, like PROLOG, miniKanren, Ohm-JS, etc.


---

Question: What is The Goal of programming?

Fodder: The Goal of programming is to control a machine using any means possible, and, to reduce our burden in doing so.  

Corollary (more fodder): fixating on a single notation, based on glyphs invented for clay-tablets, is Not a Goal of programming. At best, this is a sub-goal of programming.

Kill Math (http://worrydream.com/KillMath/)

---

Evolution of visual technologies for expressing ideas: 
1. Cave Paintings
2. Impressions in clay using pointy sticks.
3. Indelible impressions on paper using ink
4. Delible impressions on paper using graphite, erasable with rubber.
...
2020s: delible images using electronics which can express perspective and 3D-ness and overlapping rectangles, erasable on command using electro-mechanical devices.

---
I would love to hear more about your impressions of Sok Stories...
Dev tools?  Tell me more.
As Shania says "that grammar don't impress me much" :-).  Let's see, (1) to play, you need bounding boxes, you need to find all intersections of bounding boxes and then you need to pass them through an exhaustive pattern match (e.g. PROLOG, miniKanren, PEG, http://www.t3x.org/bits/prolog6.html, etc.) and (2) to specify the grammar you need rectangles (which contain bitmaps, but the computer doesn't care, as long as the images are uniquely identified) and an LHS and RHS.  The LHS says "if these sprites intersect, delete them from the canvas and add the sprites on the RHS to the canvas".  Eval all LHS rules after every tick creating the "next" canvas (double-buffering).  On starting a new tick, swap canvas-buffers and display, give control to a sprite mouser/mover/editor.  At end-of-tick, snapshot current canvas-buffer and clear next-canvas-buffer.  Apply all LHS rules to the snapshot.
[This is getting long, but, I would be glad to discuss further and/or clarify if what I said ain't yawningly obvious].

---

- split eh project into pieces
- eh: generates JSON from .drawio
- ehpy: generates Python from ../eh/???.JSON
- ehpy: contains "operating system" basics for 0D components in Python
- ehcl: contains "operating system" basics for 0D components in Common Lisp

---
The grammar, as written, tells Ohm-JS to skip spaces in Chart, Element and Command.  If a rule name begins with an upper-case letter, it is a signal to Ohm-JS to skip spaces in that rule and all of its children.  Since the first rule is Chart, spaces are disappeared before the rule Command is encountered.  Furthermore, "any" matches "#" .  The stripped input to Command is "#START#MEASURE8/9" which parses as 
```
#START#MEASURE8/9
^     ^
|     |
|     +--- value
+--- header
```

In this particular case "#" (octothorpe) is a separator and the given grammar could be rewritten as:

```
TJAGrammar {
    Chart = Element+
    Element = Command

    Command = "#" header value -- withValue
            | "#" header -- justHeader

    header = upper+
    value = (~octothorpe any)+

    octothorpe = "#"
}
```

NB. as written `header` matches any sequence of letters - which looks like a bug in the making.  One fix might be to specify each kind of header and suffix each with `~letter`, e.g.
```
header =
  | "#" "START" ~letter
  | "#" "MEASURE" ~letter
```
There are more ways to specify this thing, but there isn't enough information in the posted question to determine which answer is the "best".  For example, if you know that "value" is never going to begin with a letter, then you could stick `~letter` at the front of the `value` rule.  Or, if you know that value is always going to begin with a number, you could rewrite the rule to say that. If you are experimentally growing the grammar, then over-specifying is safer, e.g.
```
header =
  | "#START" ~letter
  | "#MEASURE" ~letter
```

If you really, really want to parse spaces, rename the rules with lower-case letters (this is just raw PEG, something which Ohm tries to help you avoid) and augment the rules to explicitly contain `spaces`, or, experiment with `applySyntactic<>`

---

I've done some playing with `applySyntactic<>`.  You are allowed to have a lower-case top rule name and to pattern-match only very specific phrases.  I built a macro parser this way.  Maybe this is the kind of thing you want...  It's on my github, but which one of the 200+ repos it is in escapes me at the moment.  If this sounds interesting, I will find the reference...