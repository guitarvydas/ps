Type checkers are helpers for development.  Similar to parsing and syntax checkers.  Type checkers are - currently - more difficult to write than syntax checkers.  There was a time when syntax checking was not well understood and was difficult and was written in an ad-hoc manner.  Early FORTRAN had syntax that we would consider weird today.  Early FORTRAN did not treat spaces specially and went for the shortest match (I think).  For example `IF` and `IFX` were both parsed as the beginning of an `IF` statement.  Later, it was discovered that making spaces "special" would help in making parsers and would help in cleaning up silliness like `IF` and `IFX`.  At the time, the character set consisted of ASCII (well, there was EBCDIC, championed by IBM, but, IBM was hated even more than Microsoft and Apple are hated today, so EBCDIC was mostly avoided by non-IBMers).  The fact that ASCII only has 128 characters to choose from (some 32 "unprintables" must be subtracted from this count) made for silly decisions like denoting strings with the same beginning and ending quote (which makes parsing more difficult) and not-allowing spaces to be embedded in names.  With Unicode, we have many more choices, but, we remain stuck with decisions made to appease 1950s hardware.  Aside: in 2022, we have hardware that can handle vector graphics and overlapping graphical elements, e.g. windows and very-small windows ("buttons", "widgets"), but, we are stuck with decisions made to appease 1950s hardware.  I argue that we should be building languages based on vector graphics instead of non-overlapping characters.  SVG is a simple example of something that might work on this front (rectangles, ellipses, lines, text, groups). Aside: "declaration before use" is a result of 1950s thinking (save CPU time by making compilers 1-pass), even though, in 2022, we could easily burn CPU cycles to figure out "declaration after use". Aside: declaration-checking (before or after use) is only a helper for developers. The machine doesn't care if you make a typo.  "Declaration-checking" is an app to help developers stamp out simple errors (like typos).  Demanding that programmers rearrange their code so that the declarations ALL come before the code is compiler-appeasement (based on 1950s hardware).

The best way to write a type checker is to use a Relational Language (like PROLOG, miniKanren, Datalog, etc., etc.).  Relational languages are shining examples of languages that don't appease compilers.  In a relational language, you write relations ("truth") and let the underlying system figure out how to implement the machinery for matching up the relations.  Other technologies that bark up this same tree: declarative languages and ML. (Aside: oh my, HTML is a declarative language.  But, HTML needs to lean on JavaScript to allow imperative break-outs).

There is no "ideal language".  The notation you use depends on the problem you are trying to solve.  A simple example would be the idea of Spreadsheets vs. Lambda Calculus.  Accountants and financial analysts like Spreadsheets.  Programming rigor analysts like Lambda Calculus.  Accountants would not want to use Lambda Calculus and rigor-ists would not want to use Spreadsheets.  Another example, closer to my heart, is the difference between using Language Theory to generate parsers and using PEG to generate parsers. Language Theory-based parsers cannot do what PEG-based parsers can do (for example, parse balanced parentheses).  Trying to force-fit language theory onto parsing has stagnated the field.  Most languages look the same, with minor differences. (Aside: PEG is Parser Theory, not Language Theory, despite the superficial simliarities in syntax).  The fact that parsing is "difficult" has restricted programmers to using only a small number of programming languages, instead of using a zillion nano-languages and defining their own nano-languages (I call these SCNs (Solution Centric Notations)).

"Dynamic" languages are "good" for fast turnaround on ideas, but are "bad" for producing end-user apps which are cost-optimized.  "Static languages" are "good" for Production Engineering apps, but, are "bad" for inventing new products.  Trying to force-fit all use-cases into one language results in a watered-down notation which isn't particularly good for either use-case.  (Aside: at the moment, efforts to force-fit all use-cases into one language favour the Production Engineering side over the Design side of things, and, this is what I call "compiler appeasement".  When programmers have to stop and rearrange their inventions to help the compiler figure out how to optimize, they are appeasing the compiler). (Aside: if Physicists *ALL* engaged in worshipping functional notation, we wouldn't have Feynman Diagrams, nor Polyani's "Order Out of Chaos", etc.).

_Barnacles_ might be invented for helping developers, e.g. type checkers and linters. Twisting a language design to appease *only* pre-compilation is *not OK* in my book. At the moment, most of our programming languages are compiler-appeasement languages and insist that developers waste time (and imagination) on dealing with compiler-appeasement and pre-compilation issues, long before the program works.

Barnacle-like pre-compilation was researched in the mid-1900s with work like Fraser/Davidson peephole technologies[1](https://github.com/guitarvydas/py0d/issues/2#user-content-fn-rtl-a08aec9667c4b8804cf824437a535475). This was called RTL and formed the basis of _gcc_. Cordy's Orthogonal Code Generator is a generalization of this technique replete with declarative syntax for a portability choice-tree (MISTs) and Data Descriptors and Condition Descriptors that improve on the virtual registers used by RTL.

